{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import wandb\n",
    "wandb.init(mode='disabled')\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from datasets import load_dataset, enable_caching\n",
    "from PIL import Image\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "from transformers import (TrainingArguments, Trainer,\n",
    "                          AutoTokenizer, AutoFeatureExtractor, AutoImageProcessor,\n",
    "                          AutoModel)  \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoImageProcessor\n",
    "\n",
    "\n",
    "\n",
    "# Assuming VITBERTVQAModel is the model class\n",
    "# loaded_model, loaded_tokenizer, loaded_processor = load_model(VITBERTVQAModel, save_directory)\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "## trai\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Collator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    img_processor: AutoFeatureExtractor\n",
    "    label_map: Dict[str, int]  \n",
    "\n",
    "    def tokenize_text(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def process_images(self, images: List[Image.Image]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        if isinstance(images, list):\n",
    "            for image in images:\n",
    "                if len(np.array(image).shape) != 3:\n",
    "                    print('oh no')\n",
    "        else:\n",
    "            if len(np.array(images).shape) != 3:\n",
    "                    print('oh no')\n",
    "                \n",
    "        processed_images = self.img_processor(\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "        ) \n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "    \n",
    "    def __call__(self, raw_batch_dict) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        if isinstance(raw_batch_dict, dict):\n",
    "            questions = raw_batch_dict['question']\n",
    "            images = raw_batch_dict['image'].convert('RGB')\n",
    "            answers = raw_batch_dict['multiple_choice_answer']# multiple_choice_answerが正解ラベル\n",
    "        else:\n",
    "            questions = [i['question'] for i in raw_batch_dict]\n",
    "            images = [i['image'] for i in raw_batch_dict]\n",
    "            answers = [i['multiple_choice_answer'] for i in raw_batch_dict]\n",
    "        \n",
    "        tokenized_texts = self.tokenize_text(questions)\n",
    "        processed_images = self.process_images(images)\n",
    "#         print(answers)\n",
    "        if isinstance(answers, str):\n",
    "            labels = self.label_map[answers] \n",
    "        else:\n",
    "#             labels = [self.label_map[answer] for answer in answers]\n",
    "            labels = [self.label_map.get(answer, self.label_map['unknown']) for answer in answers]\n",
    "        \n",
    "        return {\n",
    "            **tokenized_texts,\n",
    "            **processed_images,\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64).squeeze()\n",
    "        }\n",
    "    \n",
    "class CoAttention(nn.Module):\n",
    "    def __init__(self, text_hidden_dim, image_hidden_dim, common_hidden_dim, dropout_rate=0.3):\n",
    "        super(CoAttention, self).__init__()\n",
    "        self.common_hidden_dim = common_hidden_dim\n",
    "\n",
    "        # W_Q,W_K,W_V for text encodings\n",
    "        self.text_query = nn.Linear(text_hidden_dim, common_hidden_dim)\n",
    "        self.text_key = nn.Linear(text_hidden_dim, common_hidden_dim)\n",
    "        self.text_value = nn.Linear(text_hidden_dim, common_hidden_dim)\n",
    "        # W_Q,W_K,W_V for image encodings\n",
    "        self.image_query = nn.Linear(image_hidden_dim, common_hidden_dim)\n",
    "        self.image_key = nn.Linear(image_hidden_dim, common_hidden_dim)\n",
    "        self.image_value = nn.Linear(image_hidden_dim, common_hidden_dim)\n",
    "        \n",
    "        self.text_feed_forward = nn.Linear(common_hidden_dim + text_hidden_dim, common_hidden_dim)\n",
    "        self.image_feed_forward = nn.Linear(common_hidden_dim + image_hidden_dim, common_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        # Q,K,V for text encodings\n",
    "        Q_text = self.text_query(text_features)\n",
    "        K_text = self.text_key(text_features)\n",
    "        V_text = self.text_value(text_features)\n",
    "        # Q,K,V for image encodings\n",
    "        Q_image = self.image_query(image_features)\n",
    "        K_image = self.image_key(image_features)\n",
    "        V_image = self.image_value(image_features)\n",
    "\n",
    "        # attention weights for text encodings\n",
    "        attention_text_to_image = torch.matmul(Q_text, K_image.transpose(-2, -1)) / (self.common_hidden_dim ** 0.5)\n",
    "        attention_text_to_image = F.softmax(attention_text_to_image, dim=-1)\n",
    "        # attention weights for image encodings\n",
    "        attention_image_to_text = torch.matmul(Q_image, K_text.transpose(-2, -1)) / (self.common_hidden_dim ** 0.5)\n",
    "        attention_image_to_text = F.softmax(attention_image_to_text, dim=-1)\n",
    "        # attention scores of text encodings\n",
    "        attended_text_to_image = torch.matmul(attention_text_to_image, V_image)\n",
    "        # attention scores of image encodings\n",
    "        attended_image_to_text = torch.matmul(attention_image_to_text, V_text)\n",
    "\n",
    "        text_combined = torch.cat((text_features, attended_text_to_image), dim=-1)\n",
    "        image_combined = torch.cat((image_features, attended_image_to_text), dim=-1)\n",
    "\n",
    "        updated_text_features = self.text_feed_forward(text_combined)\n",
    "        updated_image_features = self.image_feed_forward(image_combined)\n",
    "\n",
    "        return updated_text_features, updated_image_features\n",
    "    \n",
    "    \n",
    "    \n",
    "class VITBERTcoAttentionVQAmodel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_labels: int = len(LABEL_MAP),\n",
    "            intermediate_dim: int = 512,\n",
    "            common_hidden_dim: int = 256,\n",
    "            pretrained_text_name: str = 'bert-base-uncased',\n",
    "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
    "     \n",
    "        super(VITBERTcoAttentionVQAmodel, self).__init__()\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "        \n",
    "#         self.text_encoder = AutoModel.from_pretrained(self.pretrained_text_name)\n",
    "#         self.image_encoder = AutoModel.from_pretrained(self.pretrained_image_name)\n",
    "        self.text_encoder = AutoModel.from_pretrained(self.pretrained_text_name, \n",
    "                                              hidden_dropout_prob=0.3,\n",
    "                                              attention_probs_dropout_prob=0.3)\n",
    "        self.image_encoder = AutoModel.from_pretrained(self.pretrained_image_name)\n",
    "#                                                hidden_dropout_prob=0.3,\n",
    "#                                                attention_probs_dropout_prob=0.3)\n",
    "        \n",
    "        text_hidden_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        if 'efficientnet' in self.pretrained_image_name.lower():\n",
    "            image_hidden_dim = self.image_encoder.config.num_channels\n",
    "        else:\n",
    "            image_hidden_dim = self.image_encoder.config.hidden_size\n",
    "        \n",
    "#         if 'efficientnet' in pretrained_image_name.lower():\n",
    "#             image_hidden_dim = self.image_encoder.config.num_channels\n",
    "#         else:\n",
    "#             image_hidden_dim = self.image_encoder.config.hidden_size\n",
    "        \n",
    "#         image_hidden_dim = self.image_encoder.config.hidden_size\n",
    "        self.co_attention = CoAttention(text_hidden_dim, image_hidden_dim, common_hidden_dim)\n",
    "        \n",
    "        self.text_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=common_hidden_dim, nhead=8)\n",
    "        self.text_transformer_encoder = nn.TransformerEncoder(self.text_transformer_encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.image_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=common_hidden_dim, nhead=8)\n",
    "        self.image_transformer_encoder = nn.TransformerEncoder(self.image_transformer_encoder_layer, num_layers=1)\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(common_hidden_dim * 2, intermediate_dim),  \n",
    "#             nn.BatchNorm1d(intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout層を追加\n",
    "            nn.LayerNorm(intermediate_dim)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            attention_mask: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None):\n",
    "        \n",
    "        # Encode text\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        text_features = encoded_text.last_hidden_state  \n",
    "\n",
    "        # Encode image\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        image_features = encoded_image.last_hidden_state  \n",
    "\n",
    "        # Co-attention mechanism\n",
    "        updated_text_features, updated_image_features = self.co_attention(text_features, image_features)\n",
    "\n",
    "        # Pass through respective transformer encoders\n",
    "        transformer_text_output = self.text_transformer_encoder(updated_text_features)\n",
    "        transformer_image_output = self.image_transformer_encoder(updated_image_features)\n",
    "\n",
    "        #CLS token encoding \n",
    "        cls_text_output = transformer_text_output[:, 0, :]  \n",
    "        cls_image_output = transformer_image_output[:, 0, :]  \n",
    "\n",
    "        # Fuse the CLS token encodings\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat([cls_text_output, cls_image_output], dim=1)\n",
    "        )\n",
    "\n",
    "        logits = self.classifier(fused_output)\n",
    "        \n",
    "        out = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collator_and_model(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(text)\n",
    "    processor = AutoImageProcessor.from_pretrained(image, use_fast=True)\n",
    "\n",
    "    collator = Collator(\n",
    "        tokenizer=tokenizer,\n",
    "        img_processor=processor,\n",
    "        label_map = LABEL_MAP\n",
    "    )\n",
    "\n",
    "    model = VITBERTcoAttentionVQAmodel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    return collator, model\n",
    "\n",
    "def load_model(model_class, save_directory, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # Load the state_dict\n",
    "    model_state_dict = torch.load(os.path.join(save_directory, 'model.pth'), map_location=device)\n",
    "    \n",
    "    # Initialize the model instance\n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    # Load the tokenizer and processor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    processor = AutoImageProcessor.from_pretrained(save_directory)\n",
    "    \n",
    "    print(f\"Model, tokenizer, and processor loaded from {save_directory}\")\n",
    "    \n",
    "    return model, tokenizer, processor\n",
    "\n",
    "def save_model(model, tokenizer, processor, save_directory):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    \n",
    "    # Save the model state_dict\n",
    "    torch.save(model.state_dict(), os.path.join(save_directory, 'model.pth'))\n",
    "    \n",
    "    # Save the tokenizer and processor\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    processor.save_pretrained(save_directory)\n",
    "\n",
    "    print(f\"Model, tokenizer, and processor saved to {save_directory}\")\n",
    "\n",
    "# def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "#     logits, labels = eval_tuple\n",
    "#     preds = logits.argmax(axis=-1)\n",
    "#     return {\n",
    "#         \"acc\": accuracy_score(labels, preds),\n",
    "#         \"f1\": f1_score(labels, preds, average='macro'),\n",
    "#         \"precision\": precision_score(labels, preds, average='macro', zero_division=0),\n",
    "#         \"recall\": recall_score(labels, preds, average='macro', zero_division=0)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collator_and_model(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(text)\n",
    "    processor = AutoImageProcessor.from_pretrained(image, use_fast=True)\n",
    "\n",
    "    collator = Collator(\n",
    "        tokenizer=tokenizer,\n",
    "        img_processor=processor,\n",
    "        label_map = LABEL_MAP\n",
    "    )\n",
    "\n",
    "#     model = VITBERTcoAttentionVQAmodel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    model =  MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    return collator, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "train =load_json(\"data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "train =load_json(\"data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
    "# mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "answer_list = []\n",
    "all_answers = []\n",
    "\n",
    "# 全てのIDに対して最頻値を取得\n",
    "for id, answers in train[\"answers\"].items():\n",
    "    processed_answers = [process_text(answer[\"answer\"]) for answer in answers]\n",
    "    most_common_answer = mode(processed_answers)\n",
    "    all_answers.extend(processed_answers)  # すべての回答を1次元リストに追加\n",
    "    answer_list.append({\n",
    "        \"id\": id,\n",
    "        \"most-answer\": most_common_answer\n",
    "    })\n",
    "\n",
    "# 重複を除去し、ユニークな回答のセットを作成\n",
    "unique_answers = set(all_answers)\n",
    "\n",
    "# 正解ラベルを数値に変換\n",
    "LABEL_MAP = {answer: i for i, answer in enumerate(unique_answers)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "import json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "    total_acc = 0.\n",
    "    batch_size = batch_pred.size(0)\n",
    "\n",
    "    for pred, answer in zip(batch_pred, batch_answers):\n",
    "        acc = 0.\n",
    "        pred = pred.item()  # テンソルから単一の値を取得\n",
    "        answer = answer.item()  # テンソルから単一の値を取得\n",
    "        \n",
    "        if pred == answer:\n",
    "            acc = 1.0\n",
    "        \n",
    "        total_acc += acc\n",
    "\n",
    "    return total_acc / batch_size\n",
    "\n",
    "def compute_metrics(eval_tuple):\n",
    "    logits, labels = eval_tuple\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Convert numpy arrays to torch tensors for VQA_criterion\n",
    "    predictions_tensor = torch.from_numpy(preds)\n",
    "    labels_tensor = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate VQA accuracy\n",
    "    vqa_accuracy = VQA_criterion(predictions_tensor, labels_tensor)\n",
    "    print(f\"vqa_accuracy:{vqa_accuracy}\")\n",
    "    \n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro'),\n",
    "        \"vqa_accuracy\": vqa_accuracy  # テンソルから単一の値を取得\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.metrics = {'train': [], 'eval': []}\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        # Collect training metrics\n",
    "#         print('on_epoch_end')\n",
    "        logs = state.log_history[-1] if state.log_history else {}\n",
    "        pp.pprint(logs)\n",
    "        if state.is_world_process_zero:\n",
    "            if len(state.log_history)%2 == 1: \n",
    "                self.metrics['train'].append(logs)\n",
    "            else:\n",
    "                self.metrics['eval'].append(logs)\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(f'epoch: {state.epoch}')\n",
    "\n",
    "\n",
    "\n",
    "def createAndTrainModel(dataset, args, text_model='bert-base-uncased', image_model='google/vit-base-patch16-224-in21k', multimodal_model='bert_vit',lr=0.1):\n",
    "    collator, model = create_collator_and_model(text_model, image_model)\n",
    "#     print(model)\n",
    "    multi_args = deepcopy(args)\n",
    "    multi_args.output_dir = os.path.join(\"./\", \"checkpoint\", multimodal_model)\n",
    "    metrics_callback = MetricsCallback()\n",
    "#     multi_trainer = Trainer(\n",
    "#         model,\n",
    "#         multi_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#         data_collator=collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         callbacks=[metrics_callback]\n",
    "#     )\n",
    "    from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataset) * args.num_train_epochs)\n",
    "\n",
    "    multi_trainer = Trainer(\n",
    "        model,\n",
    "        multi_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        callbacks=[metrics_callback]\n",
    "    )\n",
    "    \n",
    "#     train_multi_metrics = multi_trainer.train()\n",
    "    \n",
    "    try:\n",
    "        train_multi_metrics = multi_trainer.train()\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during training:\")\n",
    "        print(type(e).__name__, \":\", str(e))\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    \n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "    metrics_dict = metrics_callback.metrics\n",
    "    \n",
    "    # Save the model, tokenizer, and processor\n",
    "    save_directory = os.path.join(multi_args.output_dir, \"final_model\")\n",
    "    save_model(model, collator.tokenizer, collator.img_processor, save_directory)\n",
    "\n",
    "\n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics, metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, transform=None, answer=True, is_test=False, class_mapping_path=\"data/annotations/class_mapping.csv\"):\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.df = pd.read_json(df_path)\n",
    "        self.answer = answer\n",
    "        self.is_test = is_test\n",
    "\n",
    "        self.question2idx = {}\n",
    "        self.idx2question = {}\n",
    "        \n",
    "        # 新しい回答辞書の読み込み\n",
    "        cm = pd.read_csv(class_mapping_path)\n",
    "        self.answer2idx = {row[\"answer\"]: row[\"class_id\"] for _, row in cm.iterrows()}\n",
    "        self.idx2answer = {row[\"class_id\"]: row[\"answer\"] for _, row in cm.iterrows()}\n",
    "\n",
    "        for question in self.df[\"question\"]:\n",
    "            question = process_text(question)\n",
    "            words = question.split(\" \")\n",
    "            for word in words:\n",
    "                if word not in self.question2idx:\n",
    "                    self.question2idx[word] = len(self.question2idx)\n",
    "        self.idx2question = {v: k for k, v in self.question2idx.items()}\n",
    "\n",
    "        if self.answer and not self.is_test:\n",
    "            # 最頻値の回答を計算して新しい列に追加\n",
    "            self.df['multiple_choice_answer'] = self.df['answers'].apply(\n",
    "                lambda x: self._get_most_common_answer([process_text(answer[\"answer\"]) for answer in x])\n",
    "            )\n",
    "\n",
    "    def _get_most_common_answer(self, answers):\n",
    "        # 回答の頻度をカウント\n",
    "        answer_counts = Counter(answers)\n",
    "        # 最も頻度の高い回答を取得\n",
    "        most_common_answer = answer_counts.most_common(1)[0][0]\n",
    "        # 新しい回答辞書に存在する場合はそのまま返し、存在しない場合は最も近い回答を返す\n",
    "        return self._get_closest_answer(most_common_answer)\n",
    "\n",
    "    def _get_closest_answer(self, answer):\n",
    "        if answer in self.answer2idx:\n",
    "            return answer\n",
    "        # 最も近い回答を見つける（例：編集距離を使用）\n",
    "        import Levenshtein\n",
    "        closest_answer = min(self.answer2idx.keys(), key=lambda x: Levenshtein.distance(x, answer))\n",
    "        return closest_answer\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        self.question2idx = dataset.question2idx\n",
    "        self.idx2question = dataset.idx2question\n",
    "        # answer2idxとidx2answerは更新しない（新しい辞書を使用しているため）\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "        image = self.transform(image) if self.transform else image\n",
    "\n",
    "        question = self.df[\"question\"][idx]\n",
    "\n",
    "        result = {\n",
    "            'image': image,\n",
    "            'question': question\n",
    "        }\n",
    "\n",
    "        if self.answer and not self.is_test:\n",
    "            answer = self.df['multiple_choice_answer'][idx]\n",
    "            result['multiple_choice_answer'] = answer\n",
    "            result['answer_id'] = self.answer2idx[answer]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    img_processor: AutoFeatureExtractor\n",
    "\n",
    "    def tokenize_text(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def process_images(self, images: List[Image.Image]) -> Dict[str, torch.Tensor]:\n",
    "        if isinstance(images, list):\n",
    "            for image in images:\n",
    "                if len(np.array(image).shape) != 3:\n",
    "                    print('oh no')\n",
    "        else:\n",
    "            if len(np.array(images).shape) != 3:\n",
    "                print('oh no')\n",
    "                \n",
    "        processed_images = self.img_processor(\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "        ) \n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "    \n",
    "    def __call__(self, raw_batch_dict) -> Dict[str, torch.Tensor]:\n",
    "        if isinstance(raw_batch_dict, dict):\n",
    "            questions = raw_batch_dict['question']\n",
    "            images = raw_batch_dict['image'].convert('RGB')\n",
    "        else:\n",
    "            questions = [i['question'] for i in raw_batch_dict]\n",
    "            images = [i['image'] for i in raw_batch_dict]\n",
    "        \n",
    "        tokenized_texts = self.tokenize_text(questions)\n",
    "        processed_images = self.process_images(images)\n",
    "        \n",
    "        return {\n",
    "            **tokenized_texts,\n",
    "            **processed_images,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from statistics import mode\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "collator = Collator(\n",
    "        tokenizer=tokenizer,\n",
    "        img_processor=image_processor,\n",
    "        label_map = LABEL_MAP\n",
    "    )\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "#         transforms.RandomRotation(degrees=(-180, 180)),\n",
    "#         transforms.RandomCrop(32, padding=(4, 4, 4, 4), padding_mode='constant'),\n",
    "#         transforms.RandomErasing(p=0.8, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "#         transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# 元の訓練データセットを作成\n",
    "full_train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform)\n",
    "\n",
    "# テストデータセットを作成\n",
    "test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False)\n",
    "test_dataset.update_dict(full_train_dataset)\n",
    "\n",
    "# 訓練データセットを分割（例：80%訓練、20%評価）\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# データローダーの作成\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,collate_fn=collator)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collator)\n",
    "test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False,is_test=True)\n",
    "\n",
    "test_collator = TestCollator(tokenizer=collator.tokenizer, img_processor=collator.img_processor)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=test_collator)\n",
    "# テストデータローダーの作成\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "# データセットとローダーのサイズを確認\n",
    "print(f\"Full train dataset size: {len(full_train_dataset)}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Number of batches in train loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
    "print(f\"Number of batches in test loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if CUDA is available\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "import datetime\n",
    "\n",
    "# 現在の日時を取得してフォーマット\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# モデル名やその他の識別子を設定\n",
    "model_name = \"VITBERT_coAttention-test03\"\n",
    "run_identifier = f\"{model_name}_{current_time}\"\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"./checkpoints/{run_identifier}\",  # チェックポイントの基本ディレクトリ\n",
    "    run_name=run_identifier,  # 実行の固有名\n",
    "    seed=42, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=use_fp16,\n",
    "    dataloader_num_workers=4,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    "    save_steps=1,  # 1ステップごとに保存（実質的には1エポックごとに保存）\n",
    "    save_on_each_node=True,  # 分散トレーニングの場合、各ノードで保存\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start training\n",
    "# lr=を5e-5から0.01に変更\n",
    "print('='*20)\n",
    "print('begin training')\n",
    "collator2, model2, train_multi_metrics2, eval_multi_metrics2,metrics_dict2 = createAndTrainModel(train_dataset, args, text_model=\"microsoft/deberta-base\",lr=0.01)\n",
    "print('training ended')\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Starting predictions on {len(test_loader.dataset)} samples\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            # Get predicted class\n",
    "            pred_classes = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Convert to list and append to predictions\n",
    "            pred_classes = pred_classes.cpu().tolist()\n",
    "            predictions.extend(pred_classes)\n",
    "            \n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_loader):\n",
    "                print(f\"Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
    "    \n",
    "    print(\"Predictions completed\")\n",
    "    return predictions\n",
    "\n",
    "# 予測の実行\n",
    "predictions = predict_test_data(model2, test_loader)\n",
    "# 数値の予測結果を元の回答文字列に変換\n",
    "idx2answer = {v: k for k, v in collator.label_map.items()}\n",
    "predicted_answers = [idx2answer[pred] for pred in predictions]\n",
    "#　予測ファイルに変換\n",
    "submission=np.array(predicted_answers)\n",
    "np.save(\"submission-sample2.npy\", submission)\n",
    "# 以下はいらない\n",
    "# 結果の保存\n",
    "results = []\n",
    "for idx, pred_answer in enumerate(predicted_answers):\n",
    "    results.append({\n",
    "        \"question_id\": test_dataset.df['question_id'][idx],\n",
    "        \"predicted_answer\": pred_answer\n",
    "    })\n",
    "\n",
    "# 結果をJSONファイルに保存\n",
    "with open('test_predictions.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Predictions saved to test_predictions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
